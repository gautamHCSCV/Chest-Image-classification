{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bca85737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11721506816\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from sklearn.metrics import roc_curve,auc, precision_score,precision_recall_curve,recall_score,precision_recall_fscore_support,confusion_matrix\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "print(torch.cuda.is_available())\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "print(torch.cuda.get_device_properties(0).total_memory)\n",
    "print(torch.cuda.memory_allocated())\n",
    "gpu_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e72766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_class(object):\n",
    "\n",
    "    def __init__(self,py_model,gpu_id=2):\n",
    "        self.model = py_model\n",
    "        self.fpr = {}\n",
    "        self.tpr = {}\n",
    "        self.auc_ = {}\n",
    "\n",
    "        self.best_valid_acc = 0.65\n",
    "        self.best_model = None\n",
    "        self.best_optimizer = None\n",
    "\n",
    "        self.val_acc = []\n",
    "        self.train_acc = []\n",
    "        self.val_loss = []\n",
    "        self.train_loss = []\n",
    "\n",
    "        self.gpu_id=gpu_id\n",
    "        self.model = self.model.cuda(self.gpu_id)\n",
    "        \n",
    "\n",
    "    def train(self,train_loader,optimizer,Losses_,lr,n_epochs,validate_loader,schedular,regularization = 'None'):\n",
    "        weight_decay = 0.0001\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        for epochs in range(n_epochs):\n",
    "            batch_loss = []\n",
    "            batch_acc = [] \n",
    "            val_batch_loss= []\n",
    "            val_batch_acc = []\n",
    "            for idx , (data,target) in enumerate(train_loader):\n",
    "\n",
    "\n",
    "                \"\"\" mean normalization \"\"\"\n",
    "                data = self.Normalize_train(data)\n",
    "                \n",
    "                data = data.cuda(self.gpu_id)\n",
    "                target = target.cuda(self.gpu_id)\n",
    "                \n",
    "                scores = self.model(data)\n",
    "                \n",
    "                reg_loss = 0\n",
    "                if (regularization == 'l1'):\n",
    "                    for params in self.model.parameters():\n",
    "                        reg_loss += torch.sum(abs(params))\n",
    "\n",
    "                if (regularization == 'l2'):\n",
    "                    for params in self.model.parameters():\n",
    "                        reg_loss += torch.norm(params)\n",
    "                    \n",
    "                loss = Losses_(scores,target) + weight_decay * reg_loss\n",
    "                batch_loss.append(float(loss))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_accuracy = self.evaluate_batch(data,target)\n",
    "                batch_acc.append(float(batch_accuracy))\n",
    "                print(f'Train batches done: {idx+1}', end='\\r')\n",
    "                \n",
    "            self.model.eval()\n",
    "\n",
    "            for idx ,(data,target) in enumerate(validate_loader):\n",
    "\n",
    "                data = self.Normalize(data)\n",
    "\n",
    "                data = data.cuda(self.gpu_id)\n",
    "                target = target.cuda(self.gpu_id)\n",
    "\n",
    "                scores = self.model(data)\n",
    "                loss = Losses_(scores,target)\n",
    "                val_batch_loss.append(float(loss))\n",
    "\n",
    "                batch_accuracy = self.evaluate_batch(data,target)\n",
    "                val_batch_acc.append(float(batch_accuracy))\n",
    "                print(f'Validation batches done: {idx+1}', end='\\r')\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            occupied_memory = torch.cuda.memory_allocated(self.gpu_id)/(1024*1024*1024)\n",
    "            total_memory = torch.cuda.get_device_properties(self.gpu_id).total_memory/(1024*1024*1024)\n",
    "\n",
    "            self.train_loss.append(batch_loss)\n",
    "            self.train_acc.append(batch_acc)\n",
    "            self.val_loss.append(val_batch_loss)\n",
    "            self.val_acc.append(val_batch_acc)\n",
    "\n",
    "            valid_acc = np.mean(val_batch_acc)\n",
    "            if(valid_acc > self.best_valid_acc):\n",
    "                print(\"one of the best validation acc found\")\n",
    "                self.best_valid_acc = valid_acc\n",
    "                self.best_model = copy.deepcopy(self.model.state_dict())  \n",
    "                self.best_optimizer = copy.deepcopy(optimizer.state_dict())\n",
    "            \n",
    "            schedular.step()\n",
    "\n",
    "            print(f'epoch:[{epochs+1}/{n_epochs}],memory:[{occupied_memory}/{total_memory}], lr:[{optimizer.param_groups[0][\"lr\"]}]') \n",
    "            print(f'train_accuracy:{np.mean(batch_acc)}, train_loss:{np.mean(batch_loss)}')\n",
    "            print(f'val_acc:{np.mean(val_batch_acc)},val_loss:{np.mean(val_batch_loss)}')\n",
    "            print()\n",
    "                \n",
    "\n",
    "    def Normalize_train(self,data):\n",
    "        data = data/255\n",
    "        for i in range(data.shape[0]):\n",
    "            mean = torch.mean(data[i],dim = [1,2])\n",
    "            std = torch.std(data[i],dim=[1,2])\n",
    "            transform = transforms.Compose([transforms.RandomHorizontalFlip(p = 0.2),\n",
    "    #                                         transforms.RandomRotation(10),\n",
    "                                            transforms.Normalize(mean,std)])\n",
    "            data[i] = transform(data[i])\n",
    "        return data\n",
    "\n",
    "    def Normalize(self,data):\n",
    "        data = data/255\n",
    "        for i in range(data.shape[0]):\n",
    "            mean = torch.mean(data[i],dim = [1,2])\n",
    "            std = torch.std(data[i],dim=[1,2])\n",
    "            transform = transforms.Compose([transforms.Normalize(mean,std)])\n",
    "            data[i] = transform(data[i])\n",
    "        return data\n",
    "\n",
    "    def evaluate(self,loader,name='test'):\n",
    "        \n",
    "        self.model.eval()\n",
    "        correct = 0;samples =0\n",
    "\n",
    "        pre = []\n",
    "        lab = []\n",
    "        predicted_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx,(x,y) in enumerate(loader):\n",
    "\n",
    "                x = self.Normalize(x)\n",
    "                x = x.cuda(self.gpu_id)\n",
    "                y = y.cuda(self.gpu_id)\n",
    "\n",
    "                scores = self.model(x)\n",
    "                predict_prob = F.softmax(scores)\n",
    "                _,predictions = predict_prob.max(1)\n",
    "\n",
    "                predictions = predictions.to('cpu')\n",
    "                y = y.to('cpu')\n",
    "                predict_prob = predict_prob.to('cpu')\n",
    "\n",
    "                predicted_labels.extend(list(predictions.numpy()))\n",
    "                pre.extend(list(predict_prob.numpy()))\n",
    "                lab.extend(list(y.numpy()))\n",
    "\n",
    "                correct += (predictions == y).sum()\n",
    "                samples += predictions.size(0)\n",
    "\n",
    "                # torch.cuda.empty_cache(self.gpu_id)\n",
    "\n",
    "            print(f'correct are {correct}/{samples}')\n",
    "\n",
    "            lab = np.array(lab)\n",
    "            pre = np.array(pre)\n",
    "\n",
    "#         self.fpr[name],self.tpr[name],_ = roc_curve(lab,pre[:,1])\n",
    "#         self.auc_[name] = auc(self.fpr[name],self.tpr[name])\n",
    "        self.model.train()\n",
    "        return lab,pre,predicted_labels,correct/samples \n",
    "        \n",
    "\n",
    "    def evaluate_batch(self,batch,labels):\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        correct = 0;samples=0;\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = self.model(batch)\n",
    "\n",
    "            scores =F.softmax(scores,dim=1)\n",
    "            _,predicted = torch.max(scores,dim = 1)\n",
    "            correct += (predicted == labels).sum()\n",
    "            samples += scores.shape[0]\n",
    "\n",
    "            # torch.cuda.empty_cache(self.gpu_id)\n",
    "            self.model.train()\n",
    "\n",
    "        return correct/samples\n",
    "\n",
    "    def print_params(self):\n",
    "        # table = PrettyTable([\"layer\",\"parameters\"])\n",
    "\n",
    "        total_parameters = 0\n",
    "        for name,parameter in self.model.named_parameters():\n",
    "            if not parameter.requires_grad:\n",
    "                continue\n",
    "            param = parameter.numel()\n",
    "            # table.add_row([name,param])\n",
    "            total_parameters += param\n",
    "\n",
    "        # print(table)\n",
    "        print(f\"total_trainable_parameters are : {total_parameters}\")\n",
    "\n",
    "    def plot_roc(self,color,name='test'): # name can be test train or validation\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(self.fpr[name],self.tpr[name],label=f\"{name}:{self.auc_[name]}\",color=color)\n",
    "        plt.xlabel('fpr')\n",
    "        plt.ylabel('tpr')\n",
    "        plt.legend()\n",
    "        plt.title(f'{name} ROC')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def plot_precision_recall(self,lab,pre,predicted_labels,name='test'):\n",
    "\n",
    "        print(confusion_matrix(lab,predicted_labels))\n",
    "        print(\"precision : \",precision_score(lab,predicted_labels))\n",
    "        print(\"recall : \",recall_score(lab,predicted_labels))\n",
    "        precision_test,recall_test,_= precision_recall_curve(lab,pre[:,1])\n",
    "        plt.plot(recall_test,precision_test,color='red')\n",
    "        plt.xlabel(\"recall\")\n",
    "        plt.ylabel(\"precision\")\n",
    "        plt.title(f\"precision_recall_curve for {name}\")\n",
    "        plt.show()\n",
    "\n",
    "    def loss_curve(self):\n",
    "        plt.plot(list(range(1,self.n_epochs+1)),np.mean(self.train_loss,axis=1),color='orange',label='train_loss')\n",
    "        plt.plot(list(range(1,self.n_epochs+1)),np.mean(self.val_loss,axis=1),color='blue',label='validation_loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.show()\n",
    "\n",
    "    def accuracy_curve(self):\n",
    "        plt.plot(list(range(1,self.n_epochs+1)),np.mean(self.train_acc,axis=1),color='orange',label='train_acc')\n",
    "        plt.plot(list(range(1,self.n_epochs+1)),np.mean(self.val_acc,axis=1),color='blue',label='validation_acc')\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self,directory,message):\n",
    "        try:\n",
    "            torch.save({'best_model':self.best_model,'best_optimizer':self.best_optimizer,'message':message},directory)\n",
    "            print(\"model saved\")\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea2fe77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRAIN', 'TEST']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'Images/cifar-100-images/CIFAR100/'\n",
    "train_path = path + 'TRAIN'\n",
    "test_path = path+'TEST'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05ca718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(train_path)))\n",
    "s=0\n",
    "for i in os.listdir(train_path):\n",
    "    s+=len(os.listdir(train_path+'/'+i))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e643baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.transforms.Normalize(mean=torch.rand(3),std = torch.rand(3))\n",
    "transformations = torchvision.transforms.Compose([torchvision.transforms.Resize((256,256)),\n",
    "                                                  torchvision.transforms.CenterCrop(224),\n",
    "                                                  torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_images = torchvision.datasets.ImageFolder(root=train_path,transform=transformations)\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_path,transform=transformations)\n",
    "\n",
    "train_data,validate_data= torch.utils.data.dataset.random_split(train_images,[45000,5000])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=64,shuffle=True)\n",
    "validate_loader = torch.utils.data.DataLoader(dataset = validate_data,batch_size=64,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,batch_size=64,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73f9843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(train_images.class_to_idx == test_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e5fb3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), groups=16)\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), groups=16)\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), groups=32)\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), groups=32)\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), groups=48)\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), groups=48)\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), groups=64)\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), groups=64)\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Conv2d(512, 100, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "squeezenet = torchvision.models.squeezenet1_0(pretrained=True) # data again divided by 255\n",
    "\n",
    "squeezenet.features[3].expand1x1 = nn.Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1),groups=16)\n",
    "squeezenet.features[3].expand3x3 = nn.Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),groups=16)\n",
    "\n",
    "squeezenet.features[4].expand1x1 = nn.Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1),groups=16)\n",
    "squeezenet.features[4].expand3x3 = nn.Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),groups=16)\n",
    "\n",
    "squeezenet.features[5].expand1x1 = nn.Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1),groups=32)\n",
    "squeezenet.features[5].expand3x3 = nn.Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),groups=32)\n",
    "\n",
    "squeezenet.features[7].expand1x1 = nn.Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1),groups=32)\n",
    "squeezenet.features[7].expand3x3 = nn.Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),groups=32)\n",
    "\n",
    "squeezenet.features[8].expand1x1 = nn.Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1),groups=48)\n",
    "squeezenet.features[8].expand3x3 = nn.Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),groups=48)\n",
    "\n",
    "squeezenet.features[9].expand1x1 = nn.Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1),groups=48)\n",
    "squeezenet.features[9].expand3x3 = nn.Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),groups=48)\n",
    "\n",
    "squeezenet.features[10].expand1x1 = nn.Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1),groups=64)\n",
    "squeezenet.features[10].expand3x3 = nn.Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),groups=64)\n",
    "\n",
    "squeezenet.features[12].expand1x1 = nn.Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1),groups=64)\n",
    "squeezenet.features[12].expand3x3 = nn.Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),groups=64)\n",
    "\n",
    "squeezenet.classifier[1] = nn.Conv2d(512, 100, kernel_size=(1, 1), stride=(1, 1))\n",
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f8fd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'message'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = torch.load('saved_models/merged_cifar100.pth')\n",
    "check.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c211910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  tensor(0.4817)\n",
      "test accuracy:  tensor(0.4348)\n",
      "valid accuracy:  tensor(0.4186)\n"
     ]
    }
   ],
   "source": [
    "for i in ['train', 'test', 'valid']:\n",
    "    print(i+' accuracy: ',check['message'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02e82c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezenet.load_state_dict(check['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d78735db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct are 4348/10000\n",
      "Test acc: tensor(0.4348)\n",
      "correct are 21342/45000\n",
      "Train acc: tensor(0.4743)\n",
      "correct are 2429/5000\n",
      "Validation acc: tensor(0.4858)\n"
     ]
    }
   ],
   "source": [
    "model = Model_class(squeezenet,gpu_id)\n",
    "lab_test,pre_test,predict_test,acc_test=model.evaluate(test_loader,name='test')\n",
    "print('Test acc:',acc_test)\n",
    "lab_train,pre_train,predict_train,acc_train = model.evaluate(train_loader,name='train')\n",
    "print('Train acc:',acc_train)\n",
    "lab_valid,pre_valid,predict_valid,acc_valid=model.evaluate(validate_loader,name='valid')\n",
    "print('Validation acc:',acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79dba76",
   "metadata": {},
   "source": [
    "The above results is for fully trained model.\n",
    "\n",
    "Best model performance\n",
    "\n",
    "\n",
    "Below results are for model with best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89819188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['best_model', 'best_optimizer', 'message'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'saved_models/merged_cifar100_best_model.pth'\n",
    "print(torch.load(PATH).keys())\n",
    "squeezenet.load_state_dict(torch.load(PATH)['best_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95a9324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct are 4307/10000\n",
      "Test acc: tensor(0.4307)\n",
      "correct are 21287/45000\n",
      "Train acc: tensor(0.4730)\n",
      "correct are 2434/5000\n",
      "Validation acc: tensor(0.4868)\n"
     ]
    }
   ],
   "source": [
    "model = Model_class(squeezenet,gpu_id)\n",
    "lab_test,pre_test,predict_test,acc_test=model.evaluate(test_loader,name='test')\n",
    "print('Test acc:',acc_test)\n",
    "lab_train,pre_train,predict_train,acc_train = model.evaluate(train_loader,name='train')\n",
    "print('Train acc:',acc_train)\n",
    "lab_valid,pre_valid,predict_valid,acc_valid=model.evaluate(validate_loader,name='valid')\n",
    "print('Validation acc:',acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737384d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9a0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
